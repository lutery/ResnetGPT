# 当前代码中，采集样本是，其中的操作数据从哪来的？随机？
当前这段脚本里，“样本（图片 + 操作数据）”里的**操作数据来源不是单一的**，而是两路混合：

## 1）模型输出 + 随机采样（主要来源，且带随机性）
在循环里每一帧会跑一次模型：

- 用 `model(图片张量, 操作张量, trg_mask)` 得到 logits（`输出_实际_A`）
- 然后做：
  - `softmax` 转成概率分布
  - 取最后一个时间步 `输出_实际_A = 输出_实际_A[:, -1, :]`
  - 用 `torch.multinomial(输出_实际_A, num_samples=1)` **按概率随机抽一个动作 id**

也就是说：**动作 id 是从模型给出的概率分布里“随机采样”出来的**，不是纯随机均匀分布，但确实有随机性（multinomial 本身是随机采样）。

采样得到的 `抽样np[0, -1]` 会：
- 映射成指令字符串：`指令 = 数_词表[str(抽样np[0,-1])]`
- 再拆成 `移动_动作`：`指令集 = 指令.split('_')`

另外你还把这个采样出来的 id 作为“下一步输入操作序列”的一部分：

```python
操作序列 = np.append(操作序列, 抽样np[0, 0])
```

这意味着：**操作序列历史也是模型自己采样出来的结果在自回归滚动**（不是用人工标注来喂给模型）。

## 2）人工键盘输入覆盖（有输入时优先用手动）
文件顶部启动了键盘监听线程：

- `on_press` 会把按键转换成“动作名”，放进 `操作列`
- 同时 `W/A/S/D/Q` 等会改变方向状态（`W键按下`等），`Key.up` 会进入 `攻击态`

在生成要写入样本的 `操作词典` 时有个分支：

```python
方向结果 = 处理方向()
if 方向结果!='' or len(操作列)!=0 or 攻击态==True:
    # 有人工输入/方向/攻击态 => 用手动覆盖
else:
    # 没有人为干预 => 用模型采样出来的 指令集[0], 指令集[1]
```

结论：  
- **有人按键/方向/攻击态时**：样本里的`移动操作/动作操作`主要来自你的键盘输入（覆盖模型）。  
- **没人按键时**：样本里的操作就来自模型输出的概率分布 + `torch.multinomial` 的采样（因此“带随机性”）。

如果你想确认某条样本到底是“手动”还是“模型采样”，可以在写 JSON 时额外加个字段（比如 `"来源": "手动/模型"`）来标记。